{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.833224300Z",
     "start_time": "2026-01-20T22:40:27.766235100Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import torch\n",
    "import torchvision.models\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "import torch.optim as TorchOptimizers\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchinfo import summary as torch_summary\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFile\n",
    "import math\n",
    "import time\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from typing import Callable"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.840338500Z",
     "start_time": "2026-01-20T22:40:33.833224300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    train_csv_filepath: str\n",
    "    test_csv_filepath: str\n",
    "    submission_filepath: str\n",
    "    images_root_folder: str\n",
    "    image_masks_root_folder: str\n",
    "    training_output_folder: str\n",
    "    saved_weights_filepath: str\n",
    "    device: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\" For configuration variables that are shared across environments \"\"\"\n",
    "        self.num_classes = 12\n",
    "        self.batch_size = 32\n",
    "        self.starting_learning_rate = 4e-4\n",
    "        self.max_epochs = 100\n",
    "        self.patience = 20\n",
    "        self.seed = 1234\n",
    "        self.num_workers = 4\n",
    "        self.pin_memory = self.num_workers > 0 and self.device == 'cuda'\n",
    "\n",
    "    # noinspection PyAttributeOutsideInit\n",
    "    def init(self, training):\n",
    "        \"\"\" Adjust configuration setup for training vs inference \"\"\"\n",
    "        self.training = training\n",
    "\n",
    "        if self.training:\n",
    "            os.makedirs(self.training_output_folder, exist_ok=True)\n",
    "\n",
    "        self.imagenet_mean_cpu_tensor = torch.tensor(imagenet_mean_array)\n",
    "        self.imagenet_std_cpu_tensor = torch.tensor(imagenet_std_array)\n",
    "        self.channelwise_imagenet_mean_cpu_tensor = self.imagenet_mean_cpu_tensor.view(3, 1, 1)\n",
    "        self.channelwise_imagenet_std_cpu_tensor = self.imagenet_std_cpu_tensor.view(3, 1, 1)\n",
    "        self.imagenet_mean_gpu_tensor = gpu_tensor(imagenet_mean_array)\n",
    "        self.imagenet_std_gpu_tensor = gpu_tensor(imagenet_std_array)\n",
    "        self.channelwise_imagenet_mean_gpu_tensor = self.imagenet_mean_gpu_tensor.view(3, 1, 1)\n",
    "        self.channelwise_imagenet_std_gpu_tensor = self.imagenet_std_gpu_tensor.view(3, 1, 1)\n",
    "\n",
    "        self.image_transforms = T.Compose([\n",
    "            T.ToImage(),\n",
    "            T.ToDtype(torch.float32, scale=True),\n",
    "            T.Normalize(self.imagenet_mean_cpu_tensor, self.imagenet_std_cpu_tensor),\n",
    "        ])\n",
    "\n",
    "config: Config = None\n",
    "\"\"\" Set to environment-relevant config before training/inference \"\"\";"
   ],
   "id": "98d5f56f48340861",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.844850400Z",
     "start_time": "2026-01-20T22:40:33.840338500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "local_config = Config(\n",
    "    train_csv_filepath='data/train.csv',\n",
    "    test_csv_filepath='data/test.csv',\n",
    "    submission_filepath='data/submission.csv',\n",
    "    images_root_folder='data/images/',\n",
    "    image_masks_root_folder='data/masks/',\n",
    "    training_output_folder='data_gen/training_output/',\n",
    "    saved_weights_filepath='data_gen/training_output/model_weights.pth',\n",
    "    device='cpu',\n",
    ")\n",
    "kaggle_config = Config(\n",
    "    train_csv_filepath='/kaggle/input/opencv-pytorch-segmentation-project-round2/train.csv',\n",
    "    test_csv_filepath='/kaggle/input/opencv-pytorch-segmentation-project-round2/test.csv',\n",
    "    images_root_folder='/kaggle/input/opencv-pytorch-segmentation-project-round2/imgs/imgs/',\n",
    "    image_masks_root_folder='/kaggle/input/opencv-pytorch-segmentation-project-round2/masks/masks/',\n",
    "    submission_filepath='/kaggle/working/submission.csv',\n",
    "    training_output_folder='/kaggle/working/training_output/',\n",
    "    saved_weights_filepath='/kaggle/working/training_output/model_weights.pth',\n",
    "    device='cuda',\n",
    ")"
   ],
   "id": "379118cc8908681e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.849859800Z",
     "start_time": "2026-01-20T22:40:33.845858400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imagenet_mean_array = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "imagenet_std_array = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "def gpu_tensor(numpy_array):\n",
    "    return torch.tensor(numpy_array, device=config.device)\n",
    "\n",
    "def visualize_image(image_tensor):\n",
    "    \"\"\" Input tensor should be on gpu \"\"\"\n",
    "    image = denormalize(image_tensor, config.channelwise_imagenet_mean_gpu_tensor, config.channelwise_imagenet_std_gpu_tensor)\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    image = (image * 255).astype('uint8')\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_mask(mask_tensor):\n",
    "    \"\"\" Input tensor should be on gpu \"\"\"\n",
    "    mask = mask_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    mask = (mask * (255 / config.num_classes)).astype('uint8')\n",
    "    plt.imshow(mask)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def normalize(tensor, mean, std):\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    return tensor * std + mean\n",
    "\n",
    "def load_pil_image_from_id(image_id) -> ImageFile.ImageFile:\n",
    "    return Image.open(config.images_root_folder + image_id + '.jpg')"
   ],
   "id": "5fe591d8201d9401",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.855593800Z",
     "start_time": "2026-01-20T22:40:33.851161500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class ImageSegmentationDataset(TorchDataset):\n",
    "    image_ids: np.ndarray\n",
    "    image_transforms: Callable\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = Image.open(f'{config.images_root_folder}{image_id}.jpg')\n",
    "        mask = Image.open(f'{config.image_masks_root_folder}{image_id}.png')\n",
    "        return self.image_transforms(image), self.image_transforms(mask)"
   ],
   "id": "8b17b13f2b9863a5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.859514300Z",
     "start_time": "2026-01-20T22:40:33.855593800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test_model = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "# torch_summary(test_model)"
   ],
   "id": "a179e4dbb6b7be38",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.863914800Z",
     "start_time": "2026-01-20T22:40:33.859514300Z"
    }
   },
   "cell_type": "code",
   "source": "# test_model",
   "id": "6412d6b6bcba0ef0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.867919700Z",
     "start_time": "2026-01-20T22:40:33.864919500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# aux_classifier_children = test_model.aux_classifier.children()\n",
    "# for layer in aux_classifier_children:\n",
    "#     print(layer)"
   ],
   "id": "4f260a1372acb3c4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.873291600Z",
     "start_time": "2026-01-20T22:40:33.869347200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_deeplab_v3_model() -> nn.Module:\n",
    "    weights_backbone = ResNet50_Weights.DEFAULT if config.training else None\n",
    "    model = deeplabv3_resnet50(\n",
    "        weights=None,\n",
    "        weights_backbone=weights_backbone,\n",
    "        num_classes=config.num_classes,\n",
    "        aux_loss=True,\n",
    "    )\n",
    "\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.aux_classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model.to(config.device)\n",
    "\n",
    "    return model"
   ],
   "id": "c860298ad225777f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.881514100Z",
     "start_time": "2026-01-20T22:40:33.873291600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dice_coefficient_score(pred_masks, true_masks):\n",
    "    intersection = (pred_masks * true_masks).sum(dim=(1, 2))\n",
    "    union = pred_masks.sum(dim=(1, 2)) + true_masks.sum(dim=(1, 2))\n",
    "    return (2 * intersection) / union\n",
    "\n",
    "def train_one_epoch(start_time, model, loader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_pred_masks = []\n",
    "    all_true_masks = []\n",
    "\n",
    "    num_batches = math.ceil(len(loader.dataset) / config.batch_size)\n",
    "    for batch_number, (x, y) in enumerate(loader):\n",
    "        print(f't={time.time() - start_time:.2f}: Loading training batch {batch_number + 1}/{num_batches}')\n",
    "\n",
    "        x = x.to(config.device, non_blocking=True)\n",
    "        y = y.to(config.device, non_blocking=True)\n",
    "\n",
    "        if batch_number == 0:\n",
    "            allocated = torch.cuda.memory_allocated(config.device) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(config.device) / 1024**3\n",
    "            print(f'Memory allocated={allocated:.2f} GiB, reserved={reserved:.2f} GiB')\n",
    "            print(f'First image:')\n",
    "            visualize_image(x[0])\n",
    "            print(f'First mask:')\n",
    "            visualize_mask(y[0])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = loss_function(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "\n",
    "        all_pred_masks.append(preds.detach().cpu())\n",
    "        all_true_masks.append(y.detach().cpu())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    all_pred_masks = torch.cat(all_pred_masks, dim=0).numpy()\n",
    "    all_true_masks = torch.cat(all_true_masks, dim=0).numpy()\n",
    "\n",
    "    epoch_score = dice_coefficient_score(all_pred_masks, all_true_masks)\n",
    "\n",
    "    return epoch_loss, epoch_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch(start_time, model, loader, loss_function):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_pred_masks = []\n",
    "    all_true_masks = []\n",
    "    num_batches = math.ceil(len(loader.dataset) / config.batch_size)\n",
    "    for batch_number, (x, y) in enumerate(loader):\n",
    "        print(f't={time.time() - start_time:.2f}: Loading validation batch {batch_number + 1}/{num_batches}')\n",
    "\n",
    "        x = x.to(config.device, non_blocking=True)\n",
    "        y = y.to(config.device, non_blocking=True)\n",
    "\n",
    "        if batch_number == 0:\n",
    "            print('First image:')\n",
    "            visualize_image(x[0])\n",
    "            print('First mask:')\n",
    "            visualize_mask(y[0])\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_function(preds, y)\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "\n",
    "        all_pred_masks.append(preds.detach().cpu())\n",
    "        all_true_masks.append(y.detach().cpu())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    all_pred_masks = torch.cat(all_pred_masks, dim=0).numpy()\n",
    "    all_true_masks = torch.cat(all_true_masks, dim=0).numpy()\n",
    "\n",
    "    epoch_score = dice_coefficient_score(all_pred_masks, all_true_masks)\n",
    "\n",
    "    return epoch_loss, epoch_score"
   ],
   "id": "792dd9fa410a1464",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.891777500Z",
     "start_time": "2026-01-20T22:40:33.881514100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train():\n",
    "    config.init(training=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    print('t=0: Starting data prep and model loading')\n",
    "\n",
    "    run = wandb.init(\n",
    "        project='drone_image_segmentation',\n",
    "        name=f'run={int(start_time)}',\n",
    "        config={\n",
    "            'batch_size': config.batch_size,\n",
    "            'learning_rate': config.starting_learning_rate,\n",
    "            'max_epochs': config.max_epochs,\n",
    "            'seed': config.seed,\n",
    "            'model': 'deeplabv3_resnet50',\n",
    "            'optimizer': 'Adam',\n",
    "        },\n",
    "    )\n",
    "\n",
    "    train_df = pd.read_csv(config.train_csv_filepath)\n",
    "    train_array = train_df['ImageID'].to_numpy()\n",
    "    train_ids, val_ids = train_test_split(train_array, test_size=0.2, random_state=config.seed)\n",
    "\n",
    "    model = create_deeplab_v3_model()\n",
    "\n",
    "    wandb.watch(model, log='gradients', log_freq=100)\n",
    "\n",
    "    train_dataset = ImageSegmentationDataset(train_ids, config.image_transforms)\n",
    "    val_dataset = ImageSegmentationDataset(val_ids, config.image_transforms)\n",
    "\n",
    "    def loader(ds, shuffle):\n",
    "        return DataLoader(ds, shuffle=shuffle, batch_size=config.batch_size, num_workers=config.num_workers, pin_memory=config.pin_memory)\n",
    "\n",
    "    train_loader = loader(train_dataset, shuffle=True)\n",
    "    val_loader = loader(val_dataset, shuffle=False)\n",
    "\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    optimizer = TorchOptimizers.Adam(model.parameters(), lr=config.starting_learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_loss_epoch = -1\n",
    "    best_val_score = float('-inf')\n",
    "    best_val_score_epoch = -1\n",
    "\n",
    "    history = dict(train_loss=[], val_loss=[], train_score=[], val_score=[], best_val_score_epoch=dict(), best_val_loss_epoch=dict())\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    print(f't={training_start_time - start_time:.2f}: Starting training')\n",
    "    torch.manual_seed(config.seed)\n",
    "\n",
    "    best_state_dict = None\n",
    "    best_loss_state_dict = None\n",
    "    epochs_since_best = 0\n",
    "\n",
    "    for epoch in range(1, config.max_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f't={epoch_start_time - start_time:.2f}: Starting epoch {epoch}')\n",
    "        train_loss, train_score = train_one_epoch(start_time, model, train_loader, optimizer, loss_function)\n",
    "        val_loss, val_score = validate_one_epoch(start_time, model, val_loader, loss_function)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_score'].append(train_score)\n",
    "        history['val_score'].append(val_score)\n",
    "\n",
    "        print(f'================ Epoch {epoch:03d} stats ==================')\n",
    "        print(f'train_loss: {train_loss:.4f}  val_loss: {val_loss:.4f}')\n",
    "        print(f'train_score: {train_score:.4f}  val_score: {val_score:.4f}')\n",
    "        print('===================================================')\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_score': train_score,\n",
    "                'val_score': val_score,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_loss_epoch = epoch\n",
    "            best_loss_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            best_val_score_epoch = epoch\n",
    "            epochs_since_best = 0\n",
    "            best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            epochs_since_best += 1\n",
    "            if epochs_since_best >= config.patience:\n",
    "                break\n",
    "\n",
    "    history['best_val_score_epoch']['epoch'] = best_val_score_epoch\n",
    "    history['best_val_score_epoch']['val_score'] = best_val_score\n",
    "    history['best_val_loss_epoch']['epoch'] = best_val_loss_epoch\n",
    "    history['best_val_loss_epoch']['val_loss'] = best_val_loss\n",
    "\n",
    "    print()\n",
    "    print('==================== Results ======================')\n",
    "    print(f'Best val score epoch: {best_val_score_epoch}')\n",
    "    print(f'Best val score: {best_val_score:.4f}')\n",
    "    print(f'Best val loss epoch: {best_val_loss_epoch}')\n",
    "    print(f'Best val loss: {best_val_loss:.2f}')\n",
    "    print('===================================================')\n",
    "    print()\n",
    "\n",
    "    wandb.run.summary['best_val_score'] = best_val_score\n",
    "    wandb.run.summary['best_val_score_epoch'] = best_val_score_epoch\n",
    "    wandb.run.summary['best_val_loss'] = best_val_loss\n",
    "    wandb.run.summary['best_val_loss_epoch'] = best_val_loss_epoch\n",
    "\n",
    "    train_score = history['train_score']\n",
    "    val_score = history['val_score']\n",
    "    epochs = list(range(1, len(train_score) + 1))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    plt.plot(epochs, train_score, label='train_score', marker='o')\n",
    "    plt.plot(epochs, val_score, label='val_score', marker='o')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Train vs Validation Score per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    wandb.log({\"score_curve\": wandb.Image(plt.gcf())})\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    torch.save(model.state_dict(), config.training_output_folder + 'best_model_weights.pth')\n",
    "\n",
    "    model.load_state_dict(best_loss_state_dict)\n",
    "    torch.save(model.state_dict(), config.training_output_folder + 'best_loss_model_weights.pth')\n",
    "\n",
    "    with open(config.training_output_folder + 'history.json', 'w') as json_file:\n",
    "        json.dump(history, json_file, indent=4)\n",
    "\n",
    "    wandb.save(config.training_output_folder + 'best_model_weights.pth')\n",
    "    wandb.save(config.training_output_folder + 'best_loss_model_weights.pth')\n",
    "    wandb.save(config.training_output_folder + 'history.json')\n",
    "\n",
    "    wandb.finish()"
   ],
   "id": "b2fd33683b458063",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wandb_key = user_secrets.get_secret(\"wandb_key\")\n",
    "# !wandb login $wandb_key"
   ],
   "id": "451f50178e5f61bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T22:40:33.899287400Z",
     "start_time": "2026-01-20T22:40:33.892781600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = local_config\n",
    "train()"
   ],
   "id": "1e50a87e647e8e42",
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
